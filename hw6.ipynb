{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc95f30",
   "metadata": {},
   "source": [
    "# Homework 6: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bfca0",
   "metadata": {},
   "source": [
    "The goals of this assignment are:\n",
    "1. Develop a better understanding of the *self-attention mechanism* in Transformers by implementing it in numpy. \n",
    "2. Understand and train a BERT-based model. \n",
    "3. Strengthen your understanding of using HuggingFace's `transformers` package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05e52d",
   "metadata": {},
   "source": [
    "## Organization and Instructions\n",
    "Execute the code cells in Part 1 to understand the background for this assignment. You will not need to modify or add anything to Part 1. Part 2 is where your solution begins.\n",
    "\n",
    "**Part 1: Background.** \n",
    "- 1A. Environment set-up \n",
    "- 1B. Data exploration \n",
    "\n",
    "**Part 2: Your implementation.** \n",
    "- 2A. Self-attention \n",
    "- 2B. Zero-shot predictions \n",
    "- 2C. Fine-tuning \n",
    "\n",
    "\n",
    "**Addtional instructions.** \n",
    "- Please follow the 50-foot rule. Your submitted solution and code must be yours alone. Copying and pasting a solution from the internet or another source is considered a violation of the honor code. \n",
    "\n",
    "**Evaluation.** Your solution will be evaluated *manually* by the TAs and instructor. \n",
    "\n",
    "To help bridge the gap between previous homeworks and the final project. We are **not giving you an autograder**. We hope to help wean you off the grader and give you practice testing your own code.\n",
    "\n",
    "Please come see us during help hours if you need additional assistance! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f28832",
   "metadata": {},
   "source": [
    "## 1A. Environment Set-up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c8ca2",
   "metadata": {},
   "source": [
    "If you set-up your conda environment correctly in HW0, you should see `Python [conda env:cs375]` as the kernel in the upper right-hand corner of the Jupyter webpage you are currently on. Run the cell below to make sure your environment is correctly installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c48f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check \n",
    "# Return to HW0 if you run into errors in this cell \n",
    "# Do not modify this cell \n",
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs375\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77f509",
   "metadata": {},
   "source": [
    "If there are any errors after running the cell above, return to the instructions from `HW0`. If you are still having difficulty, reach out to the instructor or TAs via Piazza. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d637bf",
   "metadata": {},
   "source": [
    "#### Installing other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41206a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import typing\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          TrainingArguments, Trainer, DataCollatorWithPadding)\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util #inspect util.py to see what is in this file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee52cb",
   "metadata": {},
   "source": [
    "## 1B. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430ccd4",
   "metadata": {},
   "source": [
    "In this homework, we will use the WinoGrande dataset. This is an **extremely challenging dataset** that even a BERT-based model might have **a lot of room for performance improvements!** \n",
    "\n",
    "You can read more about the dataset in [this paper](https://cdn.aaai.org/ojs/6399/6399-13-9624-1-10-20200517.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de9f3a",
   "metadata": {},
   "source": [
    "Here is Table 1 from the WinoGrande paper with examples:  \n",
    "\n",
    "![](figs/winograd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d855501",
   "metadata": {},
   "source": [
    "HuggingFace provides a Python package for loading (and uploading datasets). You can read more about the `datasets` Python package [here](https://huggingface.co/docs/datasets/en/index). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WinoGrande dataset\n",
    "dataset = load_dataset(\"allenai/winogrande\", \"winogrande_s\", trust_remote_code=True)\n",
    "\n",
    "# Access the training and validation splits\n",
    "train_dataset = dataset[\"train\"]\n",
    "validation_dataset = dataset[\"validation\"].select(range(100)) #We'll just look at 100 dev exs\n",
    "\n",
    "print(f\"Num. train exs= {len(train_dataset)}\")\n",
    "print(f\"Num. dev exs= {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de732df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at one example from the validation dataset\n",
    "print(validation_dataset[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0a59e",
   "metadata": {},
   "source": [
    "Above, the `'sentence'` is the full sentence with a `_` for where the pronoun or noun options should go. \n",
    "\n",
    "Then `option1` and `option2` are the two token spans from the sentence the model will eventually choose from and `answer` is the correct answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07013b71",
   "metadata": {},
   "source": [
    "## 2A. Self-attention\n",
    "\n",
    "In this part, you will implement the parallelized version of the *masked* self-attention mechanism in Transformers using only numpy.\n",
    "\n",
    "\n",
    "Recall, for each layer $k$ in the transformer block we have "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58916e6f",
   "metadata": {},
   "source": [
    "For a single example with $n$ tokens and embedding dimension $d$, we first have $X^k$, the contextual embedding matrix (size $n\\times d$) for layer $k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a4705",
   "metadata": {},
   "source": [
    "Then, we introduce the weights, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d6663",
   "metadata": {},
   "source": [
    "$$ Q = X^k \\times W_Q$$ \n",
    "$$ K = X^k \\times W_K$$\n",
    "$$ V = X^k \\times W_V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b965e22",
   "metadata": {},
   "source": [
    "and use the new matrices to get the contextual embedding matrix for the next layer, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6c218",
   "metadata": {},
   "source": [
    "$$ X^{k+1} = \\text{softmax} \\bigg( \\text{mask} \\bigg( \\frac{QK^T}{\\sqrt{d}} \\bigg) \\bigg) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3600819",
   "metadata": {},
   "source": [
    "This is computationally efficient in a matrix-multiplication-optimized library like `numpy` because it should have **no for-loops!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601540b0",
   "metadata": {},
   "source": [
    "Let's implement self-attention for the (modified) example we were looking at in Part 1 \n",
    "\n",
    "*\"I had to read an entire story for class tomorrow. Luckily, it was short.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e41436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens for our example \n",
    "toks = [\"i\", \"had\", \"to\", \"read\", \"an\", \n",
    "        \"entire\", \"story\", \"for\", \"class\", \"tomorrow\", \".\",\n",
    "       \"luckily\", \"it\", \"was\", \"short\", \".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-specified embeddings and weights (for testing)\n",
    "X, W_Q, W_K, W_V = util.load_attention_data(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement your approach in this function\n",
    "\n",
    "def self_attention(X: np.ndarray, W_Q: np.ndarray, \n",
    "                   W_K: np.ndarray, W_V: np.ndarray) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    Implements (masked) self-attention mechanism for a single layer \n",
    "    (and a single example)\n",
    "    \n",
    "    Returns: X_new, a np.ndarray that is the same shape as X\n",
    "    \n",
    "    Notes:\n",
    "    - You can only use numpy for this part of the homework and no other packages\n",
    "    - Your solution must not have any for-loops!\n",
    "    \n",
    "    Tips: \n",
    "        - Double-check the shapes of all the matrices you're working with. \n",
    "        - We recommend making a helper function for the softmax.\n",
    "        - You may a subset of these numpy methods and operators helpful: `np.exp`, `@`, `np.triu_indicies`, `np.reshape`, `np.inf`, `np.broadcast_to`, `np.choose`.  \n",
    "    \"\"\"\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c1797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = self_attention(X, W_Q, W_K, W_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d36bd",
   "metadata": {},
   "source": [
    "## 2B. Zero-shot predictions\n",
    "\n",
    "Now, we will use a distilled version of \"RoBERTa\" (a BERT variant) to make zero-shot predictions on the WinoGrande dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d696c",
   "metadata": {},
   "source": [
    "#### Tokenization and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert/distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f34eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First example in the dev dataset (see dataset loading in Part 1B above)\n",
    "text1 = validation_dataset[0]['sentence']\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts to tokens and attention mask \n",
    "# The attention mask will be 0 if there are special \"PAD\" tokens\n",
    "# offset_mapping gives the start and end character index for each token in the original text \n",
    "inputs = tokenizer(text1, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5daad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the tokens \n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335bbcaf",
   "metadata": {},
   "source": [
    "Note: Above, when we see the `###` characters before tokens, this means the BERT tokenizer has split a word into subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the function below \n",
    "def which_tok_index(tok_string: str, sentence: str, inputs: torch.tensor) -> int:\n",
    "    \"\"\"\n",
    "    Inputs: the token string (tok_string) of interest, the original sentence\n",
    "    and the tokenized input tensors \n",
    "    \n",
    "    Returns: (int) the first index that matches the tok_string. \n",
    "    \n",
    "        If tok_string gets tokenized into multiple tokens, return the *first* index corresponding\n",
    "        to the multi-token span. \n",
    "\n",
    "        If there is no match (which could happen), return 0. \n",
    "    \n",
    "    Example: \n",
    "        tok_string=\"sarah\"\n",
    "        \n",
    "        input_ids = {'input_ids': tensor([[ 101, 4532, 2001, 1037, 2172, 2488, 9431, 2084, 3814, 2061, 1035, 2467,\n",
    "         2288, 1996, 6082, 3572, 1012,  102]]), \n",
    "                     'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), \n",
    "                     'offset_mapping': tensor([[[ 0,  0],\n",
    "                     [ 0,  5],\n",
    "                     [ 6,  9],\n",
    "                     [10, 11],\n",
    "                     [12, 16],\n",
    "                     [17, 23],\n",
    "                     [24, 31],\n",
    "                     [32, 36],\n",
    "                     [37, 42],\n",
    "                     [43, 45],\n",
    "                     [46, 47],\n",
    "                     [48, 54],\n",
    "                     [55, 58],\n",
    "                     [59, 62],\n",
    "                     [63, 69],\n",
    "                     [70, 75],\n",
    "                     [75, 76],\n",
    "                     [ 0,  0]]])}\n",
    "        \n",
    "        Returns: 1 \n",
    "        \n",
    "        This example returns 1 since the token id at index 1 (value 432)\n",
    "        starts at character index 6 in the orginal sentence and corresponds to \"sarah.\"\n",
    "        \n",
    "    Tips:  \n",
    "        - Understaning 'offset_mapping' and Python string methods may be helpful here\n",
    "        - tokenizer.convert_ids_to_tokens could help with debugging \n",
    "    \"\"\"\n",
    "    return 0 # delete and replace with your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test\n",
    "sent = validation_dataset[0]['sentence']\n",
    "inputs = tokenizer(sent, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "\n",
    "t1 = which_tok_index(\"sarah\", sent.lower(), inputs)\n",
    "print(t1, \"== 1?\")\n",
    "t2 = which_tok_index(\"maria\", sent.lower(), inputs)\n",
    "print(t2, \"== 8?\")\n",
    "t3 = which_tok_index(\"_\", sent.lower(), inputs)\n",
    "print(t3, \"== 10?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another unit test \n",
    "sent3 = validation_dataset[3]['sentence']\n",
    "inputs3 = tokenizer(sent3, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "\n",
    "t1 = which_tok_index(validation_dataset[3]['option1'].lower(), sent3, inputs3)\n",
    "print(t1, \"== 7?\")\n",
    "t2 = which_tok_index(validation_dataset[3]['option2'].lower(), sent3, inputs3)\n",
    "print(t2, \"== 12?\")\n",
    "t3 = which_tok_index(\"blah\", sent3, inputs3)\n",
    "print(t3, \"== 0?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02109352",
   "metadata": {},
   "source": [
    "#### Zero-shot prediction\n",
    "\n",
    "Now, we'll use the model to make zero-shot predictions. Note, this is \"zero-shot\" because we haven't ever trained the model on this particular task or dataset.  \n",
    "\n",
    "Here's how we will make zero-shot predictions: \n",
    "1. Pass the sentence (after tokenization) into the pre-trained model \n",
    "2. Obtain the final layer contextual embeddings for the `\"_\"` token as well as the *first* token for the substring of `option1` and likewise for `option2`. \n",
    "3. Find the cosine similarity between these contextual embeddings between `\"_\"` and the embedding we chose for `option1` and likewise for `option2`. \n",
    "4. Whichever has the higher cosine similarity (`option1` or `option2`), make this the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73528eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your implementation\n",
    "def zero_shot_predictions(model, tokenizer, dataset) -> List[int]: \n",
    "    \"\"\"\n",
    "    Make zero-shot predictions with the last layer contextual embedding\n",
    "    cosine similarity method described in the previous cell. \n",
    "    \n",
    "    Returns: \n",
    "        List[str], a list of strings, one element for each \n",
    "        example in the input dataset. Each element is an int: \n",
    "            - 1 corresponding to \"option1\" in the dataset\n",
    "            - 2 corresponding to \"option2\" in the dataset\n",
    "    \n",
    "    Note: \n",
    "        - For now, it's ok if you have a for-loop over examples. \n",
    "          (In an actual industry setting, you would make this all parallelized)\n",
    "        - You might make use of the `which_tok_index()` helper function \n",
    "        you just implemented \n",
    "        - The documentation on AutoModelForSequenceClassification may be helpful here. \n",
    "        - torch.nn.functional may have some helpful methods \n",
    "        - Using model.eval() and torch.no_grad() will speed things up (since Pytorch will not\n",
    "            have to make the computation graph)\n",
    "    \"\"\"\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# You might see a warning below. \n",
    "# We'll end up doing this in the next part of the homework;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ccdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code on just a single example \n",
    "zero_shot_predictions(model, tokenizer, validation_dataset.select(range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d45887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the full predictions \n",
    "preds = zero_shot_predictions(model, tokenizer, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922bac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and check the following\n",
    "# len(preds) == len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02e9e2",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daaf618",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    assert len(truth) == len(preds)\n",
    "    truth = [int(x['answer']) for x in validation_dataset]\n",
    "    y_true = np.array(truth)\n",
    "    y_pred = np.array(preds)\n",
    "    y_baseline = np.ones(len(y_true)) *2\n",
    "    print(\"F1 of baseline (maj. class)=\", np.round(f1_score(y_true, y_baseline, pos_label=2), 2))\n",
    "    print(\"F1 of zero-shot=\", np.round(f1_score(y_true, y_pred, pos_label=2), 2))\n",
    "except: \n",
    "    print(\"Need preds to be equal to truth for eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45446669",
   "metadata": {},
   "source": [
    "How does your zero-shot model compare to the baseline? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009bbae",
   "metadata": {},
   "source": [
    "##  2C. Fine-tuning\n",
    "\n",
    "Now we'll fine-tune our model on the training dataset after substituting the correct and incorrect options for \"-\", to create y=1 and y=0 options. \n",
    "\n",
    "\n",
    "We'll give you some code to help with the pre-processing. It's your job to use `TrainingArguments` and `Trainer` from HuggingFace in order to train the model. \n",
    "\n",
    "Tips:\n",
    "- [This](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) HuggingFace tutorial may be helpful for learning the transformers syntax. \n",
    "- Think **carefully** about the dataset and task when you're reasoning about the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d9df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO MODITY THIS CELL \n",
    "model_name =  \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26200325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO MODITY THIS CELL \n",
    "def preprocess_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=100)\n",
    "\n",
    "train_for_classification = Dataset.from_list(util.substitute_winograde_data(train_dataset))\n",
    "encoded_train = train_for_classification.map(preprocess_function, batched=True)\n",
    "\n",
    "dev_for_classification = Dataset.from_list(substitute_winograde_data(validation_dataset))\n",
    "encoded_dev = dev_for_classification.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31936dc6",
   "metadata": {},
   "source": [
    "TODO: It's your job to use `TrainingArguments` and `Trainer` from HuggingFace in order to train `model` we loaded above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb979bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: put your training code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198a8b1",
   "metadata": {},
   "source": [
    "Be sure to report your final F1 score on the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d72acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: put your eval code here ### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4e7dc",
   "metadata": {},
   "source": [
    "**Q: What do you attribute to the model's performance on this dataset? In other words, why did this model not get 100% accuracy?** \n",
    "\n",
    "*DELETE AND WRITE YOUR ANSWER HERE* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f93ec2",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./hw6.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. Manual solution: go to File->Download .ipynb to download your notebok and other files, then zip them locally.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip hw6.ipynb\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
