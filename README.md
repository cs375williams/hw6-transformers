# Homework 6: Transformers 

The goals of this assignment to evelop a better understanding of the self-attention mechanism in Transformers by implementing it in numpy, and strengthen your understanding of using HuggingFace's `transformers` package. 

# Getting started 

To obtain the starter code for this assignment (this repo) on your local machine, follow the same steps as [HW0](https://github.com/cs375williams/hw0-preliminaries). To recap, 

1. Open a terminal. 

2. Clone this repository into a folder of your choice via
	```
	git clone https://github.com/cs375williams/hw6-transformers
	```

3. Enter the project directory and activate your `cs375` conda environment (which you should have set-up in `HW0`): 
	```
	cd hw6-transformers
	conda activate cs375
	```

	You should now see `(cs375)` in front of your shell prompt.  

	You'll need to activate the conda environment every time you open a new terminal and re-start your notebook server. 

4. Start up your jupyter notebook server by typing in the terminal 
	```
	jupyter notebook 
	```

	A window should open automatically in your default browser. If it didn't, the terminal output should contain a URL you can use to open the notebook in a browser of your choice.


5. When you are ready to start the assignment click on `hw6.ipynb` from the Jupyter notebook file explore window that opens. Edit the notebook and craft your solutions to the assignment. 

# Submitting your work 

1. Run the final cell of `hw6.ipynb`. This should generate a zip file called `submission.zip`.

2. Upload `submission.zip` as your solution to the `Homework 6: Transformers` assignment in [CS 375's Gradescope.


