{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc95f30",
   "metadata": {},
   "source": [
    "# Homework 6: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bfca0",
   "metadata": {},
   "source": [
    "The goals of this assignment are:\n",
    "1. Develop a better understanding of the *self-attention mechanism* in Transformers by implementing it in numpy. \n",
    "2. Strengthen your understanding of using HuggingFace's `transformers` package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05e52d",
   "metadata": {},
   "source": [
    "## Organization and Instructions\n",
    "Execute the code cells in Part 1 to understand the background for this assignment. You will not need to modify or add anything to Part 1. Part 2 is where your solution begins.\n",
    "\n",
    "**Part 1: Background.** \n",
    "- 1A. Environment set-up \n",
    "- 1B. Data exploration \n",
    "\n",
    "**Part 2: Your implementation.** \n",
    "- 2A. Self-attention \n",
    "- 2B. Zero-shot predictions \n",
    "- 2C. Fine-tuning \n",
    "\n",
    "\n",
    "**Addtional instructions.** \n",
    "- Please follow the 50-foot rule. Your submitted solution and code must be yours alone. Copying and pasting a solution from the internet or another source is considered a violation of the honor code. \n",
    "\n",
    "**Evaluation.** Your solution will be evaluated *manually* by the TAs and instructor. \n",
    "\n",
    "To help bridge the gap between previous homeworks and the final project. We are **not giving you an autograder**. We hope to help wean you off the grader and give you practice testing your own code.\n",
    "\n",
    "Please come see us during help hours if you need additional assistance! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f28832",
   "metadata": {},
   "source": [
    "## 1A. Environment Set-up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c8ca2",
   "metadata": {},
   "source": [
    "If you set-up your conda environment correctly in HW0, you should see `Python [conda env:cs375]` as the kernel in the upper right-hand corner of the Jupyter webpage you are currently on. Run the cell below to make sure your environment is correctly installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c48f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check \n",
    "# Return to HW0 if you run into errors in this cell \n",
    "# Do not modify this cell \n",
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs375\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77f509",
   "metadata": {},
   "source": [
    "If there are any errors after running the cell above, return to the instructions from `HW0`. If you are still having difficulty, reach out to the instructor or TAs via Piazza. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d637bf",
   "metadata": {},
   "source": [
    "#### Installing other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41206a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import typing\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          TrainingArguments, Trainer, DataCollatorWithPadding)\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0bc86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util #inspect util.py to see what is in this file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee52cb",
   "metadata": {},
   "source": [
    "## 1B. Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730abba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430ccd4",
   "metadata": {},
   "source": [
    "In this homework, we will use the WinoGrande dataset. This is an **extremely challenging dataset** that even a BERT-based model might have **a lot of room for performance improvements!** \n",
    "\n",
    "You can read more about the dataset in [this paper](https://cdn.aaai.org/ojs/6399/6399-13-9624-1-10-20200517.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de9f3a",
   "metadata": {},
   "source": [
    "Here is Table 1 from the WinoGrande paper with examples:  \n",
    "\n",
    "![](figs/winograd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d855501",
   "metadata": {},
   "source": [
    "HuggingFace provides a Python package for loading (and uploading datasets). You can read more about the `datasets` Python package [here](https://huggingface.co/docs/datasets/en/index). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beccf83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825a7bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. train exs= 640\n",
      "Num. dev exs= 200\n"
     ]
    }
   ],
   "source": [
    "# Load the WinoGrande dataset\n",
    "dataset = load_dataset(\"allenai/winogrande\", \"winogrande_s\", trust_remote_code=True)\n",
    "\n",
    "# Access the training and validation splits\n",
    "train_dataset = dataset[\"train\"]\n",
    "validation_dataset = dataset[\"validation\"].select(range(200)) #We'll just look at 200 dev exs\n",
    "\n",
    "print(f\"Num. train exs= {len(train_dataset)}\")\n",
    "print(f\"Num. dev exs= {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de732df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'I had to read an entire story for class tomorrow. Luckily, the _ was short.', 'option1': 'story', 'option2': 'class', 'answer': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Let's look at one example from the validation dataset\n",
    "print(validation_dataset[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0a59e",
   "metadata": {},
   "source": [
    "Above, the `'sentence'` is the full sentence with a `_` for where the pronoun or noun options should go. \n",
    "\n",
    "Then `option1` and `option2` are the two token spans from the sentence the model will eventually choose from and `answer` is the correct answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07013b71",
   "metadata": {},
   "source": [
    "## 2A. Self-attention\n",
    "\n",
    "In this part, you will implement the parallelized version of the *masked* self-attention mechanism in Transformers using only numpy.\n",
    "\n",
    "\n",
    "Recall, for each layer $k$ in the transformer block we have "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58916e6f",
   "metadata": {},
   "source": [
    "For a single example with $n$ tokens and embedding dimension $d$, we first have $X^k$, the contextual embedding matrix (size $n\\times d$) for layer $k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a4705",
   "metadata": {},
   "source": [
    "Then, we introduce the weights, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d6663",
   "metadata": {},
   "source": [
    "$$ Q = X^k \\times W_Q$$ \n",
    "$$ K = X^k \\times W_K$$\n",
    "$$ V = X^k \\times W_V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b965e22",
   "metadata": {},
   "source": [
    "and use the new matrices to get the contextual embedding matrix for the next layer, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6c218",
   "metadata": {},
   "source": [
    "$$ X^{k+1} = \\text{softmax} \\bigg( \\text{mask} \\bigg( \\frac{QK^T}{\\sqrt{d}} \\bigg) \\bigg) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3600819",
   "metadata": {},
   "source": [
    "This is computationally efficient in a matrix-multiplication-optimized library like `numpy` because it should have **no for-loops!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601540b0",
   "metadata": {},
   "source": [
    "Let's implement self-attention for the (modified) example we were looking at in Part 1 \n",
    "\n",
    "*\"I had to read an entire story for class tomorrow. Luckily, it was short.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e41436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens for our example \n",
    "toks = [\"i\", \"had\", \"to\", \"read\", \"an\", \n",
    "        \"entire\", \"story\", \"for\", \"class\", \"tomorrow\", \".\",\n",
    "       \"luckily\", \"it\", \"was\", \"short\", \".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ba4bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-specified embeddings and weights (for testing)\n",
    "X, W_Q, W_K, W_V = util.load_attention_data(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8fb6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Katie's solution, TODO: delete\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    # Softmax in numpy \n",
    "    # subtract max for numerical stability\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def self_attention(X: np.ndarray, W_Q: np.ndarray, \n",
    "                   W_K: np.ndarray, W_V: np.ndarray) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    Implements self-attention mechanism for a single layer \n",
    "    (and a single example)\n",
    "    \n",
    "    Returns: X_new, a np.ndarray that is the same shape as X\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    assert W_Q.shape == (d, d)\n",
    "    \n",
    "    Q = X @ W_Q\n",
    "    V = X @ W_V \n",
    "    K = X @ W_K\n",
    "    \n",
    "    scores = (Q @ K.T)/np.sqrt(d)\n",
    "    mask = np.triu_indices(n, k=1)\n",
    "    scores[mask] = -np.inf\n",
    "    X_new = softmax(scores, axis=1) @ V\n",
    "    return X_new "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d36bd",
   "metadata": {},
   "source": [
    "## 2B. Zero-shot predictions\n",
    "\n",
    "Now, we will use a distilled version of \"RoBERTa\" (a BERT variant) to make zero-shot predictions on the WinoGrande dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d696c",
   "metadata": {},
   "source": [
    "#### Tokenization and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac9c4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert/distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2f34eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2add8122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sarah was a much better surgeon than Maria so _ always got the easier cases.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = validation_dataset[0]['sentence']\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe9c484b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 4532, 2001, 1037, 2172, 2488, 9431, 2084, 3814, 2061, 1035, 2467,\n",
       "         2288, 1996, 6082, 3572, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'offset_mapping': tensor([[[ 0,  0],\n",
       "         [ 0,  5],\n",
       "         [ 6,  9],\n",
       "         [10, 11],\n",
       "         [12, 16],\n",
       "         [17, 23],\n",
       "         [24, 31],\n",
       "         [32, 36],\n",
       "         [37, 42],\n",
       "         [43, 45],\n",
       "         [46, 47],\n",
       "         [48, 54],\n",
       "         [55, 58],\n",
       "         [59, 62],\n",
       "         [63, 69],\n",
       "         [70, 75],\n",
       "         [75, 76],\n",
       "         [ 0,  0]]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts to tokens and attention mask \n",
    "# The attention mask will be 0 if there are special \"PAD\" tokens\n",
    "inputs = tokenizer(text1, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5daad8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'sarah',\n",
       " 'was',\n",
       " 'a',\n",
       " 'much',\n",
       " 'better',\n",
       " 'surgeon',\n",
       " 'than',\n",
       " 'maria',\n",
       " 'so',\n",
       " '_',\n",
       " 'always',\n",
       " 'got',\n",
       " 'the',\n",
       " 'easier',\n",
       " 'cases',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335bbcaf",
   "metadata": {},
   "source": [
    "Note: Above, when we see the `###` characters before tokens, this means the BERT tokenizer has split a word into subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9e6ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the function below \n",
    "def which_tok_index(tok_string: str, sentence: str, inputs: torch.tensor) -> int:\n",
    "    \"\"\"\n",
    "    Inputs: the token string (tok_string) of interest, the original sentence\n",
    "    and the tokenized input tensors \n",
    "    \n",
    "    Returns: (int) the first index that matches the tok_string. \n",
    "    \n",
    "        If tok_string gets tokenized into multiple tokens, return the *first* index corresponding\n",
    "        to the multi-token span. \n",
    "\n",
    "        If there is no match (which could happen), return 0. \n",
    "    \n",
    "    Example: \n",
    "        tok_string=\"sarah\"\n",
    "        \n",
    "        input_ids = {'input_ids': tensor([[ 101, 4532, 2001, 1037, 2172, 2488, 9431, 2084, 3814, 2061, 1035, 2467,\n",
    "         2288, 1996, 6082, 3572, 1012,  102]]), \n",
    "                     'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), \n",
    "                     'offset_mapping': tensor([[[ 0,  0],\n",
    "                     [ 0,  5],\n",
    "                     [ 6,  9],\n",
    "                     [10, 11],\n",
    "                     [12, 16],\n",
    "                     [17, 23],\n",
    "                     [24, 31],\n",
    "                     [32, 36],\n",
    "                     [37, 42],\n",
    "                     [43, 45],\n",
    "                     [46, 47],\n",
    "                     [48, 54],\n",
    "                     [55, 58],\n",
    "                     [59, 62],\n",
    "                     [63, 69],\n",
    "                     [70, 75],\n",
    "                     [75, 76],\n",
    "                     [ 0,  0]]])}\n",
    "        \n",
    "        Returns: 1 \n",
    "        \n",
    "        This example returns 1 since the token id at index 1 (value 432)\n",
    "        starts at character index 6 in the orginal sentence and corresponds to \"sarah.\"\n",
    "        \n",
    "    Tips:  \n",
    "        - Understaning 'offset_mapping' and Python string methods may be helpful here\n",
    "        - tokenizer.convert_ids_to_tokens could help with debugging \n",
    "    \"\"\"\n",
    "    # find the start character\n",
    "    start_indx_goal = sentence.find(tok_string)\n",
    "    \n",
    "    for i, tok in enumerate(inputs['offset_mapping'][0]):\n",
    "        start, end = tok\n",
    "        start = start.item()\n",
    "        end = end.item()\n",
    "        if end == 0: continue \n",
    "        #see if we find a match \n",
    "        if start_indx_goal == start: return i \n",
    "    \n",
    "    return 0 # delete and replace with your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dca1f6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 == 1?\n",
      "8 == 8?\n",
      "10 == 10?\n"
     ]
    }
   ],
   "source": [
    "# Unit test\n",
    "sent = validation_dataset[0]['sentence']\n",
    "inputs = tokenizer(sent, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "\n",
    "t1 = which_tok_index(\"sarah\", sent.lower(), inputs)\n",
    "print(t1, \"== 1?\")\n",
    "t2 = which_tok_index(\"maria\", sent.lower(), inputs)\n",
    "print(t2, \"== 8?\")\n",
    "t3 = which_tok_index(\"_\", sent.lower(), inputs)\n",
    "print(t3, \"== 10?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2d4acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 == 7?\n",
      "12 == 12?\n",
      "0 == 0?\n"
     ]
    }
   ],
   "source": [
    "# Another unit test \n",
    "sent3 = validation_dataset[3]['sentence']\n",
    "inputs3 = tokenizer(sent3, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "\n",
    "t1 = which_tok_index(validation_dataset[3]['option1'].lower(), sent3, inputs3)\n",
    "print(t1, \"== 7?\")\n",
    "t2 = which_tok_index(validation_dataset[3]['option2'].lower(), sent3, inputs3)\n",
    "print(t2, \"== 12?\")\n",
    "t3 = which_tok_index(\"blah\", sent3, inputs3)\n",
    "print(t3, \"== 0?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02109352",
   "metadata": {},
   "source": [
    "#### Zero-shot prediction\n",
    "\n",
    "Now, we'll use the model to make zero-shot predictions. Note, this is \"zero-shot\" because we haven't ever trained the model on this particular task or dataset.  \n",
    "\n",
    "Here's how we will make zero-shot predictions: \n",
    "1. Pass the sentence (after tokenization) into the pre-trained model \n",
    "2. Obtain the final layer contextual embeddings for the `\"_\"` token as well as the *first* token for the substring of `option1` and likewise for `option2`. \n",
    "3. Find the cosine similarity between these contextual embeddings between `\"_\"` and the embedding we chose for `option1` and likewise for `option2`. \n",
    "4. Whichever has the higher cosine similarity (`option1` or `option2`), make this the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73528eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your implementation\n",
    "def zero_shot_predictions(model, tokenizer, dataset) -> List[int]: \n",
    "    \"\"\"\n",
    "    Make zero-shot predictions with the last layer contextual embedding\n",
    "    cosine similarity method described in the previous cell. \n",
    "    \n",
    "    Returns: \n",
    "        List[str], a list of strings, one element for each \n",
    "        example in the input dataset. Each element is an int: \n",
    "            - 1 corresponding to \"option1\" in the dataset\n",
    "            - 2 corresponding to \"option2\" in the dataset\n",
    "    \n",
    "    Note: \n",
    "        - For now, it's ok if you have a for-loop over examples. \n",
    "          (In an actual industry setting, you would make this all parallelized)\n",
    "        - You might make use of the `which_tok_index()` helper function \n",
    "        you just implemented \n",
    "        - The documentation on AutoModelForSequenceClassification may be helpful here. \n",
    "        - torch.nn.functional may have some helpful methods \n",
    "        - Using model.eval() and torch.no_grad() will speed things up (since Pytorch will not\n",
    "            have to make the computation graph)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        # Get the contextual embeddings (last hidden states)\n",
    "        text = dataset[i]['sentence']\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "        last_hidden_states = outputs.hidden_states[-1][0]\n",
    "\n",
    "        #Now get the embedding for \"_\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "        underscore_indx = which_tok_index(\"_\", text.lower(), inputs)\n",
    "        underscore_embed = last_hidden_states[underscore_indx]\n",
    "\n",
    "        # Embedding for option1\n",
    "        option1 = dataset[i]['option1'].lower()\n",
    "        option1_idx =  which_tok_index(option1, text.lower(), inputs)\n",
    "        option1_embed = last_hidden_states[option1_idx]\n",
    "\n",
    "        #Embedding for option2\n",
    "        option2 = dataset[i]['option2'].lower()\n",
    "        option2_idx =  which_tok_index(option2, text.lower(), inputs)\n",
    "        option2_embed = last_hidden_states[option2_idx]\n",
    "        \n",
    "        #print(i, \"indexes=\", underscore_indx, option1_idx, option2_idx)\n",
    "\n",
    "        #Now cosine sims \n",
    "        #print(option1, option2)\n",
    "        cos_sim1 = F.cosine_similarity(underscore_embed, option1_embed, dim=0)\n",
    "        cos_sim2 = F.cosine_similarity(underscore_embed, option2_embed, dim=0)\n",
    "\n",
    "        #Predictions\n",
    "        try: \n",
    "            if cos_sim1 > cos_sim2: \n",
    "                preds.append(1)\n",
    "            else: \n",
    "                preds.append(2)\n",
    "        except: \n",
    "            print(i, cos_sim1, cos_sim2)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ced1ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert/distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# You might see a warning below. \n",
    "# We'll end up doing this in the next part of the homework;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa4ccdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code on just a single example \n",
    "zero_shot_predictions(model, tokenizer, validation_dataset.select(range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97d45887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the full predictions \n",
    "preds = zero_shot_predictions(model, tokenizer, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "922bac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and check the following\n",
    "len(preds) == len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02e9e2",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3daaf618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 of baseline (maj. class)= 0.69\n",
      "F1 of zero-shot= 0.47\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    truth = [int(x['answer']) for x in validation_dataset]\n",
    "    assert len(truth) == len(preds)\n",
    "    y_true = np.array(truth)\n",
    "    y_pred = np.array(preds)\n",
    "    y_baseline = np.ones(len(y_true)) *2\n",
    "    print(\"F1 of baseline (maj. class)=\", np.round(f1_score(y_true, y_baseline, pos_label=2), 2))\n",
    "    print(\"F1 of zero-shot=\", np.round(f1_score(y_true, y_pred, pos_label=2), 2))\n",
    "except: \n",
    "    print(\"Need len(truth) == len(preds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45446669",
   "metadata": {},
   "source": [
    "How does your zero-shot model compare to the baseline? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009bbae",
   "metadata": {},
   "source": [
    "##  2C. Fine-tuning\n",
    "\n",
    "Now we'll fine-tune our model on the training dataset. We'll give you some code to help with the pre-processing. It's your job to use `TrainingArguments` and `Trainer` from HuggingFace in order to train the model. \n",
    "\n",
    "Tips: \n",
    "- [This](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) HuggingFace tutorial may be helpful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f93ec2",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e0b5ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found notebook file, creating submission zip...\n",
      "updating: hw6.ipynb (deflated 76%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./hw6.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. Manual solution: go to File->Download .ipynb to download your notebok and other files, then zip them locally.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip hw6.ipynb\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
